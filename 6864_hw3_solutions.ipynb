{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6864-hw3-TA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QUmbSucW8eYB"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUmbSucW8eYB"
      },
      "source": [
        "# Introduction\n",
        "In this notebook, you will find code scaffolding for the seq2seq part of Homework 3 (code for the trees section of the assignment is released in another notebook). There are certain parts of the scaffolding marked with `# Your code here!` comments where you can fill in code to perform the specified tasks. After implementing the methods in this notebook, you will need to design and perform experiments to evaluate each method and respond to the questions in the Homework 3 handout (available on Canvas). You should be able to complete this assignment without changing any of the scaffolding code, just writing code to fill in the scaffolding and run experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg6-Ub-g-T5p"
      },
      "source": [
        "# Set up dependencies and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuLyWHF--mSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "943c85f3-14cb-416c-8d97-7afa4e8dfae8"
      },
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
        "rm -rf hw3\n",
        "\n",
        "# hopefully will work when no longer private...\n",
        "# git clone https://github.com/mit-6864/hw3.git\n",
        "\n",
        "pip install sacrebleu"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (2.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gSxf6UJXqbx"
      },
      "source": [
        "!mkdir -p /content/hw3/data"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugIWLTuIYxPl",
        "outputId": "33d76f4f-229b-4e24-b221-14b24349eac0"
      },
      "source": [
        "!ls\n"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hw3  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV1H7hrZ-zgG"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/hw3\")\n",
        "\n",
        "import lab_utils\n",
        "\n",
        "import importlib\n",
        "importlib.reload(lab_utils)\n",
        "dir(lab_utils)\n",
        "\n",
        "import torch \n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\"   # use gpu whenever you can!\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q6D9jJX91Gr"
      },
      "source": [
        "# Part 1: Sequence-to-Sequence Model\n",
        "\n",
        "In this lab, we will explore RNN-based sequence-to-sequence (seq2seq) models to perform machine translation (MT).\n",
        "\n",
        "*   **Task:** translate from Vietnamese to English\n",
        "*   **Model:** RNN-based encoder-decoder\n",
        "*   **Data:** Vietnamese-English dataset from IWSLT'15\n",
        "\n",
        "\n",
        "Implementation Tasks:\n",
        "1. Data Preprocessing (done by TAs)\n",
        "2. **Encoder** \n",
        "3. **Decoder**\n",
        "4. Vanilla EncoderDecoder (done by TAs)\n",
        "5. Generator (done by TAs)\n",
        "6. **Attention Layer** (should we choose mechanism or let them choose?)\n",
        "7. **Attention-based Decoder**\n",
        "8. **EncoderAttentionDecoder** - basically can copy Vanilla\n",
        "9. Training (done by TAs)\n",
        "10. **Greedy Decoding** \n",
        "11. ***Beam Decoding (6.806 optional)***\n",
        "12. **Written/Thinking: Analyze Decoded Output**\n",
        "13. Testing via BLEU (done by TAs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHsx3v6lB3Mx"
      },
      "source": [
        "## Section 1: Data Preprocessing\n",
        "\n",
        "No need to write any code in this section. But you are encouraged to test with this part to understand the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bAN9N_lLHQs"
      },
      "source": [
        "### Download data\n",
        "First, we download the dataset and put it in the `/content/hw3/data` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTfkJNUDB_G_",
        "outputId": "ca76446d-f657-4856-ace2-6c98e8242e5b"
      },
      "source": [
        "# Download data\n",
        "DATA_DIR = \"/content/hw3/data\"\n",
        "\n",
        "!wget -nv -O \"$DATA_DIR/train.en\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
        "!wget -nv -O \"$DATA_DIR/train.vi\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
        "!wget -nv -O \"$DATA_DIR/tst2013.en\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en\n",
        "!wget -nv -O \"$DATA_DIR/tst2013.vi\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi\n",
        "!wget -nv -O \"$DATA_DIR/vocab.en\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en\n",
        "!wget -nv -O \"$DATA_DIR/vocab.vi\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-23 03:10:21 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en [13603614/13603614] -> \"/content/hw3/data/train.en\" [1]\n",
            "2021-02-23 03:10:40 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi [18074646/18074646] -> \"/content/hw3/data/train.vi\" [1]\n",
            "2021-02-23 03:10:42 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en [132264/132264] -> \"/content/hw3/data/tst2013.en\" [1]\n",
            "2021-02-23 03:10:43 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi [183855/183855] -> \"/content/hw3/data/tst2013.vi\" [1]\n",
            "2021-02-23 03:10:45 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en [139741/139741] -> \"/content/hw3/data/vocab.en\" [1]\n",
            "2021-02-23 03:10:46 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi [46767/46767] -> \"/content/hw3/data/vocab.vi\" [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqPw7tvQDIAd"
      },
      "source": [
        "### Load the Data and Preprocess\n",
        "We then load the sentences and vocab lists, only keeping sentences that do not exceed 48 words (50 with the EOS tags)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "Bah0S5rKTDT9",
        "outputId": "bf8d9d8d-a3b1-4033-f3cd-257017df6c24"
      },
      "source": [
        "from lab_utils import read_vocab_file, read_sentence_file, filter_data, show_some_data_stats\n",
        "\n",
        "src_vocab_set = read_vocab_file(\"vocab.vi\")\n",
        "trg_vocab_set = read_vocab_file(\"vocab.en\")\n",
        "\n",
        "train_src_sentences_list = read_sentence_file(\"train.vi\")\n",
        "train_trg_sentences_list = read_sentence_file(\"train.en\")\n",
        "assert len(train_src_sentences_list) == len(train_trg_sentences_list)\n",
        "\n",
        "test_src_sentences_list = read_sentence_file(\"tst2013.vi\")\n",
        "test_trg_sentences_list = read_sentence_file(\"tst2013.en\")\n",
        "assert len(test_src_sentences_list) == len(test_trg_sentences_list)\n",
        "\n",
        "# Filter out sentences over 48 words long\n",
        "MAX_SENT_LENGTH = 48\n",
        "MAX_SENT_LENGTH_PLUS_SOS_EOS = 50\n",
        "\n",
        "train_src_sentences_list, train_trg_sentences_list = filter_data(\n",
        "    train_src_sentences_list, train_trg_sentences_list, MAX_SENT_LENGTH)\n",
        "test_src_sentences_list, test_trg_sentences_list = filter_data(\n",
        "    test_src_sentences_list, test_trg_sentences_list, MAX_SENT_LENGTH)\n",
        "\n",
        "# We take 10% of training data as validation set.\n",
        "num_val = int(len(train_src_sentences_list) * 0.1)\n",
        "val_src_sentences_list = train_src_sentences_list[:num_val]\n",
        "val_trg_sentences_list = train_trg_sentences_list[:num_val]\n",
        "train_src_sentences_list = train_src_sentences_list[num_val:]\n",
        "train_trg_sentences_list = train_trg_sentences_list[num_val:]\n",
        "\n",
        "show_some_data_stats(train_src_sentences_list, val_src_sentences_list, \n",
        "                     test_src_sentences_list, train_trg_sentences_list,\n",
        "                     src_vocab_set, trg_vocab_set)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training (src, trg) sentence pairs: 108748\n",
            "Number of validation (src, trg) sentence pairs: 12083\n",
            "Number of testing (src, trg) sentence pairs: 1139\n",
            "Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 7710\n",
            "Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 17192\n",
            "Training sentence avg. length: 20 \n",
            "Training sentence length at 95-percentile: 42\n",
            "Training sentence length distribution (x-axis is length range and y-axis is count):\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Example Vietnamese input: ['Adam', 'Sadowsky', 'dàn', 'dựng', '1', 'video', 'âm', 'nhạc', 'hiện', 'tượng', '.']\n",
            "Its target English output: ['Adam', 'Sadowsky', ':', 'How', 'to', 'engineer', 'a', 'viral', 'music', 'video']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAk-fj8GEdOj"
      },
      "source": [
        "## **Section 2: Encoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfutdpgxrxa5"
      },
      "source": [
        "First, here is the Dataset. The IDs that we reserve might be useful later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOjSiD3urw_R"
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "# These IDs are reserved.\n",
        "##TODO ABBY __ OFF BY ONE ERROR< ADD PAD TO VOCABS! LINE ZEOR\n",
        "PAD_INDEX = 0\n",
        "UNK_INDEX = 1\n",
        "SOS_INDEX = 2\n",
        "EOS_INDEX = 3\n",
        "\n",
        "\n",
        "class MTDataset(data.Dataset):\n",
        "  def __init__(self, src_sentences, src_vocabs, trg_sentences, trg_vocabs,\n",
        "               sampling=1.):\n",
        "    self.src_sentences = src_sentences[:int(len(src_sentences) * sampling)]\n",
        "    self.trg_sentences = trg_sentences[:int(len(src_sentences) * sampling)]\n",
        "\n",
        "    self.max_src_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n",
        "    self.max_trg_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n",
        "\n",
        "    self.src_vocabs = src_vocabs\n",
        "    self.trg_vocabs = trg_vocabs\n",
        "\n",
        "    self.src_v2id = {v : i for i, v in enumerate(src_vocabs)}\n",
        "    self.src_id2v = {val : key for key, val in self.src_v2id.items()}\n",
        "    self.trg_v2id = {v : i for i, v in enumerate(trg_vocabs)}\n",
        "    self.trg_id2v = {val : key for key, val in self.trg_v2id.items()}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.src_sentences)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    src_sent = self.src_sentences[index]\n",
        "    src_len = len(src_sent) + 2   # add <s> and </s> to each sentence\n",
        "    src_id = []\n",
        "    for w in src_sent:\n",
        "      if w not in self.src_vocabs:\n",
        "        w = '<unk>'\n",
        "      src_id.append(self.src_v2id[w])\n",
        "    src_id = ([SOS_INDEX] + src_id + [EOS_INDEX] + [PAD_INDEX] *\n",
        "              (self.max_src_seq_length - src_len))\n",
        "\n",
        "    trg_sent = self.trg_sentences[index]\n",
        "    trg_len = len(trg_sent) + 2\n",
        "    trg_id = []\n",
        "    for w in trg_sent:\n",
        "      if w not in self.trg_vocabs:\n",
        "        w = '<unk>'\n",
        "      trg_id.append(self.trg_v2id[w])\n",
        "    trg_id = ([SOS_INDEX] + trg_id + [EOS_INDEX] + [PAD_INDEX] *\n",
        "              (self.max_trg_seq_length - trg_len))\n",
        "\n",
        "    return torch.tensor(src_id), src_len, torch.tensor(trg_id), trg_len"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzQu_1Y5sZEb"
      },
      "source": [
        "\n",
        "Seq2seq consists of an Encoder RNN and a Decoder RNN. In a vanilla seq2seq model where there is no attention mechanism between encoder and decoder, the encoder aims to compress the information contained in the entire input sequence into a single vector and pass it to decoder.\n",
        "\n",
        "We start with implementing the encoder, which is just a simple RNN. We use a GRU here, but feel free to try other cell types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ3rJ1TlElu5"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "      - `input_size`: an int representing the RNN input size.\n",
        "      - `hidden_size`: an int representing the RNN hidden size.\n",
        "      - `dropout`: a float representing the dropout rate during training. Note\n",
        "          that for 1-layer RNN this has no effect since dropout only applies to\n",
        "          outputs of intermediate layers.\n",
        "    \"\"\"\n",
        "    \n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True,\n",
        "                      dropout=dropout, bidirectional=False)\n",
        "\n",
        "  def forward(self, inputs, lengths):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of source\n",
        "          sentences.\n",
        "      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n",
        "          lengths of `inputs`.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "        (batch_size, max_seq_length, hidden_size).\n",
        "      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n",
        "      Hint: `outputs` and `finals` are both standard GRU outputs. Check:\n",
        "      https://pytorch.org/docs/stable/nn.html#gru\n",
        "    \"\"\"\n",
        "    outputs = None\n",
        "    finals = None\n",
        "    \n",
        "    # --------- Your code here --------- #\n",
        "    packed = pack_padded_sequence(inputs, lengths.cpu(), batch_first=True,\n",
        "                                  enforce_sorted=False)\n",
        "    outputs, finals = self.rnn(packed)\n",
        "    outputs, _ = pad_packed_sequence(outputs, batch_first=True,\n",
        "                                     total_length=MAX_SENT_LENGTH_PLUS_SOS_EOS)\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return outputs, finals"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCTTreYNFMja"
      },
      "source": [
        "## **Section 3: Decoder**\n",
        "\n",
        "Here you will implement a decoder RNN that uses encoder's last hidden state to initialize its initial hidden state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZnDk1j9FNOH"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  \"\"\"An RNN decoder without attention.\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "    \"\"\"\n",
        "      Inputs:\n",
        "        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "    \"\"\"\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    # --------- Your code here --------- #\n",
        "    self.rnn = nn.GRU(input_size, hidden_size, batch_first=True,\n",
        "                      dropout=dropout)\n",
        "\n",
        "    # To initialize from the final encoder state.\n",
        "    self.bridge = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "\n",
        "    self.dropout_layer = nn.Dropout(p=dropout)\n",
        "    self.pre_output_layer = nn.Linear(hidden_size + input_size, hidden_size,\n",
        "                                      bias=False)\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "  def forward_step(self, prev_embed, hidden):\n",
        "    \"\"\"Perform a single decoder step (1 word).\"\"\"\n",
        "\n",
        "    # Update RNN hidden state.\n",
        "    output, hidden = self.rnn(prev_embed, hidden)\n",
        "\n",
        "    pre_output = torch.cat([prev_embed, output], dim=2)\n",
        "\n",
        "    pre_output = self.dropout_layer(pre_output)\n",
        "    pre_output = self.pre_output_layer(pre_output)\n",
        "\n",
        "    return output, hidden, pre_output\n",
        "\n",
        "  def forward(self, inputs, encoder_finals, hidden=None, max_len=None):\n",
        "    \"\"\"Unroll the decoder one step at a time.\n",
        "\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of target\n",
        "          sentences (for teacher-forcing during training).\n",
        "      - `encoder_finals`: a 3d-tensor of shape\n",
        "          (num_enc_layers, batch_size, hidden_size) representing the final\n",
        "          encoder hidden states used to initialize the initial decoder hidden\n",
        "          states.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the value to be used to initialize the initial decoder hidden states.\n",
        "          If None, then use `encoder_finals`.\n",
        "      - `max_len`: an int representing the maximum decoding length.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the raw\n",
        "          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n",
        "          We will convert it later in a `Generator` below.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the last decoder hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # The maximum number of steps to unroll the RNN.\n",
        "    if max_len is None:\n",
        "      max_len = inputs.size(1)\n",
        "\n",
        "    # Initialize decoder hidden state.\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(encoder_finals)\n",
        "\n",
        "    outputs = None\n",
        "    \n",
        "    # --------- Your code ends --------- #\n",
        "    # Here we store all intermediate hidden states and pre-output vectors.\n",
        "    decoder_states = []\n",
        "    pre_output_vectors = []\n",
        "\n",
        "    # Unroll the decoder RNN for `max_len` steps.\n",
        "    for i in range(max_len):\n",
        "      prev_embed = inputs[:, i].unsqueeze(1)\n",
        "      output, hidden, pre_output = self.forward_step(prev_embed, hidden)\n",
        "      decoder_states.append(output)\n",
        "      pre_output_vectors.append(pre_output)\n",
        "\n",
        "    decoder_states = torch.cat(decoder_states, dim=1)\n",
        "    outputs = torch.cat(pre_output_vectors, dim=1)\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return hidden, outputs\n",
        "\n",
        "  def init_hidden(self, encoder_finals):\n",
        "    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n",
        "    state.\"\"\"\n",
        "    decoder_init_hiddens = None\n",
        "\n",
        "    # --------- Your code here (hint: only one line needed) --------- #\n",
        "    decoder_init_hiddens = torch.tanh(self.bridge(encoder_finals))\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return decoder_init_hiddens"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rZKppmtF6rr"
      },
      "source": [
        "Define the high level encoder-decoder class to wrap up sub-models, including encoder, decoder, generator, and src/trg embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPmfDZ2fF7jy"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"A standard Encoder-Decoder architecture without attention.\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `encoder`: an `Encoder` object.\n",
        "      - `decoder`: a `Decoder` object.\n",
        "      - `src_embed`: an nn.Embedding object representing the lookup table for\n",
        "          input (source) sentences.\n",
        "      - `trg_embed`: an nn.Embedding object representing the lookup table for\n",
        "          output (target) sentences.\n",
        "      - `generator`: a `Generator` object. Essentially a linear mapping. See\n",
        "          the next code cell.\n",
        "    \"\"\"\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src_ids, trg_ids, src_lengths):\n",
        "    \"\"\"Take in and process masked source and target sequences.\n",
        "\n",
        "    Inputs:\n",
        "      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of source sentences of word ids.\n",
        "      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of target sentences of word ids.\n",
        "      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n",
        "        sequence length of `src_ids`.\n",
        "\n",
        "    Returns the decoder outputs, see the above cell.\n",
        "    \"\"\"\n",
        "    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n",
        "    del encoder_hiddens   # unused\n",
        "    return self.decode(encoder_finals, trg_ids[:, :-1])\n",
        "\n",
        "  def encode(self, src_ids, src_lengths):\n",
        "    return self.encoder(self.src_embed(src_ids), src_lengths)\n",
        "    \n",
        "  def decode(self, encoder_finals, trg_ids, decoder_hidden=None):\n",
        "    return self.decoder(self.trg_embed(trg_ids), encoder_finals, decoder_hidden)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWKrjqzqGA-F"
      },
      "source": [
        "It simply projects the pre-output layer (x in the forward function below) to obtain the output layer, so that the final dimension is the target vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T43dUgwLGBnI"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "  def __init__(self, hidden_size, vocab_size):\n",
        "    super(Generator, self).__init__()\n",
        "    self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvEjGmojGD0-"
      },
      "source": [
        "## **Section 4: Attention-Based Decoder**\n",
        "\n",
        "Now it's time to add some attention to the decoder. We make the classes again.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR5luqfmGIJ8"
      },
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "  \"\"\"An attention-based RNN decoder.\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, attention=None, dropout=0.):\n",
        "    \"\"\"\n",
        "      Inputs:\n",
        "        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "        - `attention`: this is your self-defined Attention object. You can\n",
        "            either define an individual class for your Attention and pass it\n",
        "            here or leave `attention` as None and just implement everything\n",
        "            here.\n",
        "    \"\"\"\n",
        "    super(AttentionDecoder, self).__init__()\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "    self.attention = attention\n",
        "\n",
        "    self.rnn = nn.GRU(input_size + hidden_size, hidden_size, batch_first=True,\n",
        "                      dropout=dropout)\n",
        "\n",
        "    # To initialize from the final encoder state.\n",
        "    self.bridge = nn.Linear(hidden_size, hidden_size, bias=True)\n",
        "\n",
        "    self.dropout_layer = nn.Dropout(p=dropout)\n",
        "    self.pre_output_layer = nn.Linear(hidden_size + hidden_size + input_size,\n",
        "                                      hidden_size, bias=False)\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "  ## START ANSWER // PART OF ANSWER BELOW\n",
        "  def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key,\n",
        "                   hidden):\n",
        "    \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
        "\n",
        "    # Compute context vector using attention mechanism.\n",
        "    query = hidden[-1].unsqueeze(1)\n",
        "\n",
        "    context, attn_probs = self.attention(query=query, proj_key=proj_key,\n",
        "                                         value=encoder_hidden, mask=src_mask)\n",
        "\n",
        "    # Update RNN hidden state.\n",
        "    rnn_input = torch.cat([prev_embed, context], dim=2)\n",
        "    output, hidden = self.rnn(rnn_input, hidden)\n",
        "\n",
        "    pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
        "\n",
        "    pre_output = self.dropout_layer(pre_output)\n",
        "    pre_output = self.pre_output_layer(pre_output)\n",
        "\n",
        "    return output, hidden, pre_output\n",
        "  ## END ANSWER\n",
        "\n",
        "  def forward(self, inputs, encoder_hiddens, encoder_finals,  src_mask,\n",
        "              trg_mask, hidden=None, max_len=None):\n",
        "    \"\"\"Unroll the decoder one step at a time.\n",
        "    \n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of target\n",
        "          sentences (for teacher-forcing during training).\n",
        "      - `encoder_hiddens`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the encoder\n",
        "          outputs for each decoding step to attend to. \n",
        "      - `encoder_finals`: a 3d-tensor of shape\n",
        "          (num_enc_layers, batch_size, hidden_size) representing the final\n",
        "          encoder hidden states used to initialize the initial decoder hidden\n",
        "          states.\n",
        "      - `src_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length)\n",
        "          representing the mask for source sentences.\n",
        "      - `trg_mask`: a 3d-tensor of shape (batch_size, 1, max_seq_length)\n",
        "          representing the mask for target sentences.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the value to be used to initialize the initial decoder hidden states.\n",
        "          If None, then use `encoder_finals`.\n",
        "      - `max_len`: an int representing the maximum decoding length.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: (same as in Decoder) a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the raw\n",
        "          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the last decoder hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # The maximum number of steps to unroll the RNN.\n",
        "    if max_len is None:\n",
        "      max_len = trg_mask.size(-1)\n",
        "\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(encoder_finals)\n",
        "\n",
        "    outputs = None\n",
        "    \n",
        "    # --------- Your code here --------- #\n",
        "    # Pre-compute projected encoder hidden states (the \"keys\" for the attention\n",
        "    # mechanism). This is only done for efficiency.\n",
        "    proj_key = self.attention.key_layer(encoder_hiddens)\n",
        "\n",
        "    # Here we store all intermediate hidden states and pre-output vectors.\n",
        "    decoder_states = []\n",
        "    pre_output_vectors = []\n",
        "\n",
        "    # Unroll the decoder RNN for `max_len` steps.\n",
        "    for i in range(max_len):\n",
        "      prev_embed = inputs[:, i].unsqueeze(1)\n",
        "      output, hidden, pre_output = self.forward_step(prev_embed,\n",
        "                                                     encoder_hiddens, src_mask,\n",
        "                                                     proj_key, hidden)\n",
        "      decoder_states.append(output)\n",
        "      pre_output_vectors.append(pre_output)\n",
        "\n",
        "    decoder_states = torch.cat(decoder_states, dim=1)\n",
        "    outputs = torch.cat(pre_output_vectors, dim=1)\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return hidden, outputs\n",
        "\n",
        "  def init_hidden(self, encoder_finals):\n",
        "    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n",
        "    state.\"\"\"\n",
        "    decoder_init_hiddens = None\n",
        "    # --------- Your code here --------- #\n",
        "    decoder_init_hiddens = torch.tanh(self.bridge(encoder_finals))\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return decoder_init_hiddens"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yz2FU1DG57j"
      },
      "source": [
        "Similarly, we use a `EncoderAttentionDecoder` class to wrap up all encoder, decoder, src/trg embeddings, and generator. You can take the `EncoderDecoder` class as a reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR85lqb1G71e"
      },
      "source": [
        "class EncoderAttentionDecoder(nn.Module):\n",
        "  \"\"\"A Encoder-Decoder architecture with attention.\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed , trg_embed, generator):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `encoder`: an `Encoder` object.\n",
        "      - `decoder`: an `AttentionDecoder` object.\n",
        "      - `src_embed`: an nn.Embedding object representing the lookup table for\n",
        "          input (source) sentences.\n",
        "      - `trg_embed`: an nn.Embedding object representing the lookup table for\n",
        "          output (target) sentences.\n",
        "      - `generator`: a `Generator` object. Essentially a linear mapping. See\n",
        "          the next code cell.\n",
        "    \"\"\"\n",
        "    super(EncoderAttentionDecoder, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src_ids, trg_ids, src_lengths):\n",
        "    \"\"\"Take in and process masked source and tar get sequences.\n",
        "\n",
        "    Inputs:\n",
        "      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of source sentences of word ids.\n",
        "      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of target sentences of word ids.\n",
        "      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n",
        "        sequence length of `src_ids`.\n",
        "\n",
        "    Returns the decoder outputs, see the above cell.\n",
        "    \"\"\"\n",
        "    # --------- Your code here --------- #\n",
        "    # Hint: You can refer to `EncoderDecoder` and extend from it.\n",
        "    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n",
        "    src_mask = (src_ids != PAD_INDEX).unsqueeze(-2)\n",
        "    trg_mask = (trg_ids[:, 1:] != PAD_INDEX).unsqueeze(-2)\n",
        "    return self.decode(encoder_hiddens, encoder_finals, src_mask,\n",
        "                       trg_ids[:, :-1], trg_mask)\n",
        "\n",
        "  def encode(self, src_ids, src_lengths):\n",
        "    return self.encoder(self.src_embed(src_ids), src_lengths)\n",
        "\n",
        "  def decode(self, encoder_hiddens, encoder_finals, src_mask, trg_ids,\n",
        "             trg_mask, decoder_hidden=None):\n",
        "    return self.decoder(self.trg_embed(trg_ids), encoder_hiddens,\n",
        "                        encoder_finals, src_mask, trg_mask, decoder_hidden)\n",
        "  # --------- Your code ends --------- #"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hz_Vw7CHVCy"
      },
      "source": [
        "Attention layer plays a key role in seq2seq models. It allows for capturing the dependencies without the distance constraints in the input or output sequences. In this lab, your goal is to implement the Bahdanau attention mechanisms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0WxyImrHTUQ"
      },
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "  \"\"\"Implements Bahdanau (MLP) attention.\"\"\"\n",
        "\n",
        "  def __init__(self, hidden_size, key_size=None, query_size=None):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "\n",
        "    # --------- Your code here --------- #\n",
        "    key_size = hidden_size if key_size is None else key_size\n",
        "    query_size = hidden_size if query_size is None else query_size\n",
        "\n",
        "    self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
        "    self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
        "    self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    # To store attention scores.\n",
        "    self.alphas = None\n",
        "    # --------- Your code ends --------- #\n",
        "        \n",
        "  def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
        "    assert mask is not None, \"mask is required\"\n",
        "\n",
        "    # --------- Your code here --------- #\n",
        "\n",
        "    # We first project the query (the decoder state).\n",
        "    # The projected keys (the encoder states) were already pre-computated.\n",
        "    query = self.query_layer(query)\n",
        "\n",
        "    # Calculate scores.\n",
        "    scores = self.energy_layer(torch.tanh(query + proj_key))\n",
        "    scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "    # Mask out invalid positions.\n",
        "    # The mask marks valid positions so we invert it using `mask & 0`.\n",
        "    scores.data.masked_fill_(mask == 0, -float('inf'))\n",
        "\n",
        "    # Turn scores to probabilities.\n",
        "    alphas = F.softmax(scores, dim=-1)\n",
        "    self.alphas = alphas        \n",
        "\n",
        "    # The context vector is the weighted sum of the values.\n",
        "    context = torch.bmm(alphas, value)\n",
        "\n",
        "    # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
        "    return context, alphas\n",
        "    # --------- Your code ends --------- #"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdfQQaXgHcX_"
      },
      "source": [
        "## **Section 5: Training**\n",
        "\n",
        "We provide training and testing scripts here. You might need to adapt them to fit your model implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcaY3sxDHkdq"
      },
      "source": [
        "Apply the dataloader to the MTDataset, which is defined in `lab_utils.py`. Dataloader provides a convenient way to iterate through the whole dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_PXmw3HgXo"
      },
      "source": [
        "from lab_utils import MTDataset\n",
        "from torch.utils import data\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# You can try on a smaller training set by setting a smaller `sampling`.\n",
        "train_set = MTDataset(train_src_sentences_list, src_vocab_set,\n",
        "                      train_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "train_data_loader = data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                    num_workers=8, shuffle=True)\n",
        "\n",
        "val_set = MTDataset(val_src_sentences_list, src_vocab_set,\n",
        "                    val_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "val_data_loader = data.DataLoader(val_set, batch_size=batch_size, num_workers=8,\n",
        "                                  shuffle=False)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PVjxBbyHr6p"
      },
      "source": [
        "The main functions for training, here we use perplexity to evaluate the performance of the model. Although we provide the training scripts here, we strongly encoureage you to go through and understand the procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIrMBzl7HswK"
      },
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class SimpleLossCompute:\n",
        "  \"\"\"A simple loss compute and train function.\"\"\"\n",
        "\n",
        "  def __init__(self, generator, criterion, opt=None):\n",
        "    self.generator = generator\n",
        "    self.criterion = criterion\n",
        "    self.opt = opt\n",
        "\n",
        "  def __call__(self, x, y, norm):\n",
        "    x = self.generator(x)\n",
        "    loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
        "                          y.contiguous().view(-1))\n",
        "    loss = loss / norm\n",
        "\n",
        "    if self.opt is not None:  # training mode\n",
        "      loss.backward()          \n",
        "      self.opt.step()\n",
        "      self.opt.zero_grad()\n",
        "\n",
        "    return loss.data.item() * norm\n",
        "\n",
        "\n",
        "def run_epoch(data_loader, model, loss_compute, print_every):\n",
        "  \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "  total_tokens = 0\n",
        "  total_loss = 0\n",
        "\n",
        "  for i, (src_ids_BxT, src_lengths_B, trg_ids_BxL, trg_lengths_B) in enumerate(data_loader):\n",
        "    # We define some notations here to help you understand the loaded tensor\n",
        "    # shapes:\n",
        "    #   `B`: batch size\n",
        "    #   `T`: max sequence length of source sentences\n",
        "    #   `L`: max sequence length of target sentences; due to our preprocessing\n",
        "    #        in the beginning, `L` == `T` == 50\n",
        "    # An example of `src_ids_BxT` (when B = 2):\n",
        "    #   [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n",
        "    #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n",
        "    # The corresponding `src_lengths_B` would be [47, 49].\n",
        "    # Note that SOS_INDEX == 2, EOS_INDEX == 3, and PAD_INDEX = 0.\n",
        "\n",
        "    # ## Weird error with pytorch(https://github.com/pytorch/pytorch/issues/16542)\n",
        "    # torch.set_default_tensor_type(torch.FloatTensor)\n",
        "\n",
        "    src_ids_BxT = src_ids_BxT.to(device)\n",
        "    src_lengths_B = src_lengths_B.to(device)\n",
        "    trg_ids_BxL = trg_ids_BxL.to(device)\n",
        "\n",
        "    \n",
        "# padded_track_seq = pack_padded_sequence(sorted_tracks, sorted_n_tracks, batch_first=True, enforce_sorted=True)\n",
        "# if self.device == torch.device(\"cuda\"):\n",
        "#     torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "# padded_track_seq.to(self.device)\n",
        "\n",
        "\n",
        "    del trg_lengths_B   # unused\n",
        "\n",
        "    _, output = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n",
        "\n",
        "    loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n",
        "                        norm=src_ids_BxT.size(0))\n",
        "    total_loss += loss\n",
        "    total_tokens += (trg_ids_BxL[:, 1:] != lab_utils.PAD_INDEX).data.sum().item()\n",
        "\n",
        "    if model.training and i % print_every == 0:\n",
        "      print(\"Epoch Step: %d Loss: %f\" % (i, loss / src_ids_BxT.size(0)))\n",
        "\n",
        "  return math.exp(total_loss / float(total_tokens))\n",
        "\n",
        "\n",
        "def train(model, num_epochs, learning_rate, print_every):\n",
        "  # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n",
        "  # computing the loss.\n",
        "  criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=lab_utils.PAD_INDEX)\n",
        "  optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Keep track of dev ppl for each epoch.\n",
        "  dev_ppls = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(\"Epoch\", epoch)\n",
        "\n",
        "    model.train()\n",
        "    train_ppl = run_epoch(data_loader=train_data_loader, model=model,\n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, optim),\n",
        "                          print_every=print_every)\n",
        "        \n",
        "    model.eval()\n",
        "    with torch.no_grad():      \n",
        "      dev_ppl = run_epoch(data_loader=val_data_loader, model=model,\n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, None),\n",
        "                          print_every=print_every)\n",
        "      print(\"Validation perplexity: %f\" % dev_ppl)\n",
        "      dev_ppls.append(dev_ppl)\n",
        "        \n",
        "  return dev_ppls"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZduMNu6GHyRn"
      },
      "source": [
        "The main function to perform training. First let's train the vanilla seq2seq model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "eE0h-FwcHzL5",
        "outputId": "c0de6df0-6e36-4a5f-ce60-7b15bdaaabe3"
      },
      "source": [
        "# Hyperparameters for contructing the encoder-decoder model.\n",
        "embed_size = 256   # Each word will be represented as a `embed_size`-dim vector.\n",
        "hidden_size = 256  # RNN hidden size.\n",
        "dropout = 0.2\n",
        "\n",
        "pure_seq2seq = EncoderDecoder(\n",
        "  encoder=Encoder(embed_size, hidden_size, dropout=dropout),\n",
        "  decoder=Decoder(embed_size, hidden_size, dropout=dropout),\n",
        "  src_embed=nn.Embedding(len(src_vocab_set), embed_size),\n",
        "  trg_embed=nn.Embedding(len(trg_vocab_set), embed_size),\n",
        "  generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n",
        "\n",
        "# Start training. The returned `dev_ppls` is a list of dev perplexity for each\n",
        "# epoch.\n",
        "#TODO@ABBY FIX THIS MAKE BIGGER WHEN NO DEBUG\n",
        "pure_dev_ppls = train(pure_seq2seq, num_epochs=2, learning_rate=1e-3,\n",
        "                      print_every=100)\n",
        "\n",
        "# Plot perplexity\n",
        "lab_utils.plot_perplexity(pure_dev_ppls)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 0 Loss: 173.498230\n",
            "Epoch Step: 100 Loss: 88.926224\n",
            "Epoch Step: 200 Loss: 90.446068\n",
            "Epoch Step: 300 Loss: 79.464531\n",
            "Epoch Step: 400 Loss: 83.588844\n",
            "Epoch Step: 500 Loss: 80.984108\n",
            "Epoch Step: 600 Loss: 81.790985\n",
            "Epoch Step: 700 Loss: 72.918243\n",
            "Epoch Step: 800 Loss: 71.780922\n",
            "Validation perplexity: 71.275738\n",
            "Epoch 1\n",
            "Epoch Step: 0 Loss: 69.527634\n",
            "Epoch Step: 100 Loss: 71.573280\n",
            "Epoch Step: 200 Loss: 79.081505\n",
            "Epoch Step: 300 Loss: 71.394035\n",
            "Epoch Step: 400 Loss: 69.688110\n",
            "Epoch Step: 500 Loss: 65.221878\n",
            "Epoch Step: 600 Loss: 62.120903\n",
            "Epoch Step: 700 Loss: 69.616058\n",
            "Epoch Step: 800 Loss: 63.899509\n",
            "Validation perplexity: 52.244962\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLs1c2hPH6wS"
      },
      "source": [
        "Now, let's train the seq2seq model with attention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQb1QXqJH8lf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "1ef0fb64-d05c-4941-b9a0-3d4fa6140b1c"
      },
      "source": [
        "attn_seq2seq = EncoderAttentionDecoder(\n",
        "  encoder=Encoder(embed_size, hidden_size, dropout=dropout),\n",
        "  decoder=AttentionDecoder(embed_size, hidden_size,\n",
        "                  attention=BahdanauAttention(hidden_size), dropout=dropout),\n",
        "  src_embed=nn.Embedding(len(src_vocab_set), embed_size),\n",
        "  trg_embed=nn.Embedding(len(trg_vocab_set), embed_size),\n",
        "  generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n",
        "\n",
        "attn_dev_ppls = train(attn_seq2seq, num_epochs=1, learning_rate=1e-3,\n",
        "                      print_every=100)\n",
        "\n",
        "lab_utils.plot_perplexity(attn_dev_ppls)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 0 Loss: 174.313354\n",
            "Epoch Step: 100 Loss: 97.797432\n",
            "Epoch Step: 200 Loss: 82.022820\n",
            "Epoch Step: 300 Loss: 82.155258\n",
            "Epoch Step: 400 Loss: 70.289207\n",
            "Epoch Step: 500 Loss: 67.859009\n",
            "Epoch Step: 600 Loss: 67.797462\n",
            "Epoch Step: 700 Loss: 70.868988\n",
            "Epoch Step: 800 Loss: 60.844482\n",
            "Validation perplexity: 32.377419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlAJiAeuH-H4"
      },
      "source": [
        "## Section 6: Decoding Algorithms\n",
        "\n",
        "Now that we have a trained model, the next task is to decode the model output. This is non-trivial. We will first implement a naive, greedy approach, and then we will investigate beam search. You are only required to implement decoding algorithms for the pure_seq2seq model, as a few implementation details will be different if you include Attention. \n",
        "\n",
        "1. greedy search\n",
        "2. beam search (6.864 only, optional for 6.806)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLMRgd8cMx1n"
      },
      "source": [
        "### Greedy Decode\n",
        "\n",
        "For greedy decoding, we will generate (or \"decode\") the target sentence by simply taking the argmax on each step of the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABoDgeCmIAII"
      },
      "source": [
        "def greedy_decode(model, src_ids, src_lengths, max_len):\n",
        "  \"\"\"Greedily decode a sentence for EncoderDecoder. Make sure to chop off the \n",
        "     EOS token!\"\"\"\n",
        "\n",
        "  with torch.no_grad():\n",
        "    _, encoder_finals = model.encode(src_ids, src_lengths)\n",
        "    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
        "\n",
        "  output = []\n",
        "  hidden = None\n",
        "\n",
        "  # --------- Your code here --------- #\n",
        "  for i in range(max_len):\n",
        "    with torch.no_grad():\n",
        "      hidden, outputs = model.decode(encoder_finals, prev_y, hidden)\n",
        "      prob = model.generator(outputs[:, -1])\n",
        "\n",
        "    d, next_word = torch.max(prob, dim=1)\n",
        "    next_word = next_word.data.item()\n",
        "    output.append(next_word)\n",
        "    prev_y = torch.ones(1, 1).type_as(src_ids).fill_(next_word)\n",
        "\n",
        "  output = np.array(output)\n",
        "\n",
        "  # Cut off everything starting from </s>.\n",
        "  first_eos = np.where(output == EOS_INDEX)[0]\n",
        "  if len(first_eos) > 0:\n",
        "    output = output[:first_eos[0]]\n",
        "  # --------- Your code ends --------- #\n",
        "\n",
        "  return output\n"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BskyqhTwICct"
      },
      "source": [
        "Print the top 3 examples from the data loader by applying the greedy decoder (using the validation dataset).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1qGxjybIISL",
        "outputId": "5cd50c63-27bb-4cbc-87de-2e3314f13015"
      },
      "source": [
        "example_set = MTDataset(val_src_sentences_list, src_vocab_set,\n",
        "                        val_trg_sentences_list, trg_vocab_set)\n",
        "example_data_loader = data.DataLoader(example_set, batch_size=1, num_workers=1,\n",
        "                                      shuffle=False)\n",
        "\n",
        "\n",
        "lab_utils.print_examples(pure_seq2seq, src_vocab_set, trg_vocab_set,\n",
        "                         example_data_loader, greedy_decode)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example #1\n",
            "Src :  \n",
            "Trg :  \n",
            "Pred:  The second of the next year is a very different way .\n",
            "\n",
            "Example #2\n",
            "Src :  Tôi muốn cho các bạn biết về sự to lớn của những nỗ lực khoa học đã góp phần làm nên các dòng tít bạn thường thấy trên báo .\n",
            "Trg :  I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n",
            "Pred:  I want to tell you about the most important thing that you &apos;re talking about the most important thing to be the most important thing .\n",
            "\n",
            "Example #3\n",
            "Src :  Có những dòng trông như thế này khi bàn về biến đổi khí hậu , và như thế này khi nói về chất lượng không khí hay khói bụi .\n",
            "Trg :  <unk> that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n",
            "Pred:  There are the same thing that &apos;s going to be the right now , and it &apos;s not the same thing to do this , and it &apos;s the same thing to do it .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlhc3gMAO965"
      },
      "source": [
        "### Beam Search Decode \n",
        "Greedy search is easy to implement and fast, but it might not return the optimal translation. We could also try an exhaustive search, but that might take a LONG time. Beam search allows us to constrain the hypothesis search space in a smart way, allowing for greater translation quality while still maintaining a semblance of efficiency.\n",
        "\n",
        "Students taking 6.806 are only required to do the basic, OG version. Those taking 6.864 must implement the OG version and also normalize by length.\n",
        "\n",
        "::sidenote\n",
        "Here's a relevant paper, 'Beam Search Strategies for Neural Machine Translation' [(Freitag and Al-Onaizan 2017)](https://arxiv.org/pdf/1702.01806.pdf) for some newer thoughts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpI8daVnWhvF"
      },
      "source": [
        "#### OG Beam\n",
        "\n",
        "The original beam-search strategy finds a translation that approximately maximizes the conditional probability given by a specific model. It builds the translation from left-to-right and keeps a fixed number (beam) of translation candidates with the highest log-probability at each time step. For each end-of-sequence symbol that is selected among the highest scoring candidates the beam is reduced by one and the translation is stored into a final candidate list. When the beam is zero, it stops the search and picks the translation with the highest log-probability out of the final candidate list.\n",
        "\n",
        "[Hint: Beam Search with k=1 is the exact same thing as greedy search.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0HQwrZ-Iys2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8900f89b-6cb4-43f5-981f-bbc5f8293dc2"
      },
      "source": [
        "def og_beam_search(model, src_ids, src_lengths, max_len, beam_width=5):\n",
        "  \"\"\"Use beam search to decode a sentence for EncoderDecoder.\"\"\"\n",
        "\n",
        "  with torch.no_grad():\n",
        "    _, encoder_finals = model.encode(src_ids, src_lengths)\n",
        "    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
        "\n",
        "  output = None\n",
        "  # --------- Your code here --------- #\n",
        "\n",
        "  # keeps track of the top beam size hypotheses to expand for each element\n",
        "  # in the batch to be further decoded (that are still \"alive\")\n",
        "  new_sequences = [[[], None, prev_y, 0.0]]\n",
        "\n",
        "  # Give full probability to the first beam on the first step.\n",
        "  topk_log_probs = [float(\"-inf\") for _ in range(beam_width)]\n",
        "\n",
        "  # Structure that holds finished hypotheses.\n",
        "  hypotheses = [[] for _ in range(batch_size)]\n",
        "\n",
        "  hidden = None\n",
        "\n",
        "  # walk over each step in sequence\n",
        "  for i in range(max_len):\n",
        "    all_candidates = []\n",
        "    # go through each candidate\n",
        "    sequences = new_sequences[:]\n",
        "    for i in range(len(sequences)):\n",
        "      seq, hidden, prev_y, beam_log_prob = sequences[i]\n",
        "\n",
        "      # if already at EOS then don't expand more, but keep in running\n",
        "      if prev_y[0][0] == EOS_INDEX:\n",
        "        all_candidates.append([seq, hidden, prev_y,beam_log_prob])\n",
        "        continue\n",
        "\n",
        "      # expand current candidate and get log_probs\n",
        "      with torch.no_grad():\n",
        "        hidden, outputs = model.decode(encoder_finals, prev_y, hidden)\n",
        "        log_probs = model.generator(outputs[:, -1])\n",
        "\n",
        "      # multiply probs by the current beam probability (=add logprobs)\n",
        "      log_probs += beam_log_prob\n",
        "\n",
        "      # get k best from this beam\n",
        "      k_probs, k_next_words = torch.topk(log_probs, beam_width, dim=1)\n",
        "      for m in range(len(k_next_words[0])):\n",
        "        next_word = k_next_words[0][m]\n",
        "        next_prob = k_probs[0][m]\n",
        "\n",
        "        current_seq = seq[:]\n",
        "\n",
        "        next_word_int = next_word.data.item()\n",
        "        current_seq.append(next_word_int)\n",
        "      \n",
        "        prev_y = torch.ones(1, 1).type_as(src_ids).fill_(next_word_int)\n",
        "        all_candidates.append([current_seq, hidden, prev_y,next_prob])\n",
        "\n",
        "    # continue with only only best k\n",
        "    new_sequences = sorted(all_candidates, key=lambda x: x[-1], reverse=True)[:beam_width]\n",
        "\n",
        "  # remove </s> from end of each\n",
        "  output = [np.array(seq[0])[:-1] for seq in new_sequences]\n",
        "  # --------- Your code ends --------- #\n",
        "\n",
        "  return output\n",
        "\n",
        "lab_utils.print_examples(pure_seq2seq, src_vocab_set, trg_vocab_set,\n",
        "                         example_data_loader, og_beam_search, beam_width=3)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example #1\n",
            "Src :  \n",
            "Trg :  \n",
            "Pred #1:  Now , let &apos;s take a look at the bottom .\n",
            "Pred #2:  Now , let &apos;s take a look at the top of the world .\n",
            "Pred #3:  Now , let &apos;s take a look at the top of the brain .\n",
            "\n",
            "Example #2\n",
            "Src :  Tôi muốn cho các bạn biết về sự to lớn của những nỗ lực khoa học đã góp phần làm nên các dòng tít bạn thường thấy trên báo .\n",
            "Trg :  I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n",
            "Pred #1:  I want to tell you about the most important part of the world , because the most important part of the world is .\n",
            "Pred #2:  I want to tell you about the most important part of the world , because the most important part of the world is really .\n",
            "Pred #3:  I want to tell you about the most important part of the world , because the most important part of the world is really quite .\n",
            "\n",
            "Example #3\n",
            "Src :  Có những dòng trông như thế này khi bàn về biến đổi khí hậu , và như thế này khi nói về chất lượng không khí hay khói bụi .\n",
            "Trg :  <unk> that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n",
            "Pred #1:  There are these two things that are going to be done , and it &apos;s not going to be in the middle of the right now , and it &apos;s going to be the right now .\n",
            "Pred #2:  There are these two things that are going to be done , and it &apos;s not going to be in the middle of the right now , and it &apos;s going to be in the way .\n",
            "Pred #3:  There are these two things that are going to be done , and it &apos;s not going to be in the middle of the right now , and it &apos;s going to be in the way to the right .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKkexRkjWrW5"
      },
      "source": [
        "#### Beam Extensions\n",
        "\n",
        "Beam has many issues. One is that longer hypotheses have lower scores.\n",
        "\n",
        "* **Length normalization:** length_penalty(Y)=(5+|Y|)^α/(5+1)^α where |Y| is the current hypothesis length and α is the length normalization coefficient (as defined in [Wu et al. 2016](https://arxiv.org/abs/1609.08144)).\n",
        "* **End of sentence normalization:** eos_penalty(X,Y)= γ *|X|/|Y| where |X| is the source length, |Y| is the current target length and γ is the end of sentence normalization coefficient\n",
        "* **Hypothesis Filtering:** drop hypothesis with more X unknown words \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRibSPP5Z7RB"
      },
      "source": [
        "def beam_plus_search(model, src_ids, src_lengths, max_len, beam_width=5):\n",
        "  \"\"\"Use beam search to decode a sentence for EncoderDecoder.\"\"\"\n",
        "  #TODO if desired\n",
        "  return None\n",
        "\n",
        "lab_utils.print_examples(pure_seq2seq, src_vocab_set, trg_vocab_set, \n",
        "                         example_data_loader, beam_plus_search, beam_width=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ji_jPyNIJrN"
      },
      "source": [
        "## Section 7: Testing\n",
        "Compute the BLEU score. BLEU score is a standard measure to evaluate the translation results. For further details, you can refer to [this](https://en.wikipedia.org/wiki/BLEU) link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5q6hAcKILyW",
        "outputId": "7dd58116-1530-4c28-c33f-9f2a86304450"
      },
      "source": [
        "test_set = MTDataset(test_src_sentences_list, src_vocab_set,\n",
        "                     test_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "test_data_loader = data.DataLoader(test_set, batch_size=1, num_workers=8,\n",
        "                                   shuffle=False)\n",
        "\n",
        "print('BLEU score: %f' % (np.mean(lab_utils.compute_BLEU(pure_seq2seq,\n",
        "                                                         test_data_loader,\n",
        "                                                         greedy_decode,\n",
        "                                                         trg_vocab_set))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 444/1139 [00:14<00:21, 31.91it/s]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}